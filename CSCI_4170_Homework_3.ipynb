{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiden-dm/CSCI-4170/blob/main/CSCI_4170_Homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mh_6KzKjx5B"
      },
      "source": [
        "#**Projects in AI and ML - Homework 3**\n",
        "##**Aiden Drover-Mattinen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X5HaYm0Ai76"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUxeaceakedJ"
      },
      "source": [
        "For this assignment, I chose to tackle the classic problem of identifying handwritten digits from images using the MNIST database. This dataset includes 60,000 training samples and 10,000 test samples of size-normalized handwritten digits. I selected this well-known and relatively clean dataset because I wanted to focus on understanding how neural networks work, without getting bogged down by complex, messy data.\n",
        "\n",
        "This is my first experience working with neural networks, and I initially found them quite confusing. By choosing a straightforward problem, I aimed to simplify the learning process and build a strong foundational understanding. Additionally, since this is a computer vision problem, it complements the computer vision class I am currently taking at RPI. The skills I develop here will not only deepen my understanding of neural networks but also enhance my performance in that class.\n",
        "\n",
        "Link to dataset: https://www.kaggle.com/datasets/hojjatk/mnist-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports for code"
      ],
      "metadata": {
        "id": "eFIsYsXgCgCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_nR7KN2JRVis"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuT5SHoGnMjM"
      },
      "source": [
        "## Implementing a neural network from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6JQlgl6nVtp"
      },
      "source": [
        "Conveniently, the MNIST dataset can be loaded directly from the Keras library as I've done below. It comes already divided into a training and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qLY0lquJcndn"
      },
      "outputs": [],
      "source": [
        "# Loading MNIST dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-IV7snomyF"
      },
      "source": [
        "Despite the data being relatively clean, some preprocessing was still necessary before using it in my neural network.\n",
        "\n",
        "First, I reshaped the data so that each example was represented as a single column vector, with the number of rows corresponding to the number of training examples. Since the images are normalized to 28x28 pixels, I flattened each one into a column vector of length 784, where each pixel serves as a feature in the model.\n",
        "\n",
        "Next, I normalized the pixel intensity values to a range between 0 and 1. As the images are in grayscale, dividing each pixel value by 255 achieved this normalization. Neural networks typically train more efficiently on normalized data.\n",
        "\n",
        "Lastly, I performed one-hot encoding on the target labels. This process transforms the labels into column vectors of length 10, filled with zeros except for a single one at the index corresponding to the digit label. For example, a label of 5 would be represented as:\n",
        "\n",
        "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0].T\n",
        "\n",
        "I performed this one-hot encoding because the softmax activation function (which I will justify shortly) outputs probabilities across multiple classes, and one-hot encoding allows for a direct comparison between predicted outputs and true labels during model training.\n",
        "\n",
        "Sources:\n",
        " * https://medium.com/towards-data-science/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQVyFYCpc89_",
        "outputId": "0b37573a-8274-4435-bbf9-2e670c9f8f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (784, 60000)\n",
            "Y_train: (10, 60000)\n",
            "X_test:  (784, 10000)\n",
            "Y_test:  (10, 10000)\n"
          ]
        }
      ],
      "source": [
        "# Ensure that the training data is in the shape (features, examples)\n",
        "train_X = train_X.reshape(train_X.shape[0], -1).T\n",
        "test_X = test_X.reshape(test_X.shape[0], -1).T\n",
        "\n",
        "# Normalize light intensities in grayscale image\n",
        "test_X = test_X / 255.0\n",
        "train_X = train_X / 255.0\n",
        "\n",
        "# Will use softmax, so y values will be one-hot encoded\n",
        "train_y = to_categorical(train_y, num_classes=10).T\n",
        "test_y = to_categorical(test_y, num_classes=10).T\n",
        "\n",
        "# Print out the shapes to confirm they are correct\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRRfoRtrUyJ"
      },
      "source": [
        "Next I had to select and define the activation functions to be used in my network. I used the ReLU (Rectified Linear Unit) activation function in the hidden layer and the softmax activation function in the output layer.\n",
        "\n",
        "ReLU is widely used because of its simplicity and effectiveness. It introduces non-linearity to the model, allowing it to learn complex patterns in the data. ReLU also helps mitigate the vanishing gradient problem, enabling faster and more efficient training by allowing gradients to propagate without diminishing quickly.\n",
        "\n",
        "Softmax is ideal for multi-class classification problems like digit recognition because it converts the raw output scores into probabilities that sum to 1. This makes it easier to interpret the model’s predictions, as each output node represents the likelihood of the input image corresponding to a particular digit.\n",
        "\n",
        "Sources:\n",
        "* https://builtin.com/machine-learning/relu-activation-function#:~:text=What%20is%20ReLU%20used%20for,more%20complex%20relationships%20in%20data.\n",
        "* https://www.analyticsvidhya.com/blog/2021/04/introduction-to-softmax-for-neural-network/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tll0Ff-NrWxh"
      },
      "outputs": [],
      "source": [
        "# Define the ReLU function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define the derivative of the ReLU function to\n",
        "# be used in the backpropagation algorithm\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF1htZJetFAm"
      },
      "source": [
        "My next step was to write the neural network class. I implemented a network with a single hidden layer as it made the implementation more simple. The following is justification for the decisions to use He initialization for my weights, using mini-batch gradient descent, and using the Adam optimizer.\n",
        "\n",
        "**He Initialization**:\n",
        "He initialization was used to improve the performance of ReLU activation functions. By scaling weights with $\\sqrt{\\frac{2}{\\text{input size}}}$, it maintains variance through layers, preventing vanishing or exploding gradients, and facilitating faster convergence.\n",
        "\n",
        "**Mini-Batch Gradient Descent**:\n",
        "Mini-batch gradient descent balances efficiency and stability. It processes subsets of data, reducing memory load and providing faster updates than full-batch, while introducing enough noise to help escape local minima compared to stochastic gradient descent. I hoped that this strategy would overcome any inefficiencies of writing the neural network myself.\n",
        "\n",
        "**Adam Optimizer**:\n",
        "The Adam optimizer combines momentum and adaptive learning rates, enabling efficient, stable convergence.\n",
        "\n",
        "For my loss function, I chose **categorical cross entropy** because it is well-suited for multi-class classification tasks where the target labels are one-hot encoded. The categorical cross-entropy loss function effectively measures the discrepancy between the predicted probability distribution and the actual distribution of these one-hot encoded labels, facilitating efficient training of the neural network.\n",
        "\n",
        "Sources:\n",
        "* https://medium.com/@shauryagoel/kaiming-he-initialization-a8d9ed0b5899\n",
        "* https://www.geeksforgeeks.org/adam-optimizer/\n",
        "* https://medium.com/@shireenchand/choosing-between-cross-entropy-and-sparse-cross-entropy-the-only-guide-you-need-abea92c84662"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ImUXHGS4Ulre"
      },
      "outputs": [],
      "source": [
        "# Create the neural network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
        "        # Initialize biases\n",
        "        self.b1 = np.zeros((hidden_size, 1))\n",
        "        self.b2 = np.zeros((output_size, 1))\n",
        "\n",
        "        # He initialization for weights for better ReLU performance\n",
        "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2. / input_size)\n",
        "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2. / hidden_size)\n",
        "\n",
        "        # Adam optimizer parameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "        # Initialize momentum values\n",
        "        self.m_W1 = np.zeros_like(self.W1)\n",
        "        self.m_b1 = np.zeros_like(self.b1)\n",
        "        self.m_W2 = np.zeros_like(self.W2)\n",
        "        self.m_b2 = np.zeros_like(self.b2)\n",
        "\n",
        "        # Initialize velocity values\n",
        "        self.v_W1 = np.zeros_like(self.W1)\n",
        "        self.v_b1 = np.zeros_like(self.b1)\n",
        "        self.v_W2 = np.zeros_like(self.W2)\n",
        "        self.v_b2 = np.zeros_like(self.b2)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward_propagation(self, X):\n",
        "        # Create linear combination and apply ReLU for hidden layer\n",
        "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
        "        self.A1 = relu(self.Z1)\n",
        "\n",
        "        # Create linear combination and apply softmax for output layer\n",
        "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
        "        self.A2 = softmax(self.Z2)\n",
        "\n",
        "        return self.A2\n",
        "\n",
        "    # Define the loss function (categorical cross entropy for this case)\n",
        "    def categorical_cross_entropy(self, y_true, y_pred):\n",
        "        # Small value added to avoid log(0)\n",
        "        epsilon = 1e-12\n",
        "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]\n",
        "        return loss\n",
        "\n",
        "    # Backward propagation function\n",
        "    def backward_propagation(self, X, Y, Y_hat):\n",
        "        # Get the number of samples\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # Calculate derivatives for output layer\n",
        "        dZ2 = Y_hat - Y\n",
        "        dW2 = np.dot(dZ2, self.A1.T) / m\n",
        "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "\n",
        "        # Calculate derivatives for hidden layer\n",
        "        dZ1 = (np.dot(self.W2.T, dZ2) * relu_derivative(self.Z1))\n",
        "        dW1 = np.dot(dZ1, X.T) / m\n",
        "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "        # Return all derivatives (gradients)\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    # Adam optimizer update\n",
        "    def adam_optimizer(self, dW1, db1, dW2, db2):\n",
        "\n",
        "        # Store gradients and corresponding moment and velocity references\n",
        "        gradients = [dW1, db1, dW2, db2]\n",
        "        moments = [self.m_W1, self.m_b1, self.m_W2, self.m_b2]\n",
        "        velocities = [self.v_W1, self.v_b1, self.v_W2, self.v_b2]\n",
        "        parameters = [self.W1, self.b1, self.W2, self.b2]\n",
        "\n",
        "        # Update moment and velocity estimates\n",
        "        for i in range(len(gradients)):\n",
        "            moments[i] = self.beta1 * moments[i] + (1 - self.beta1) * gradients[i]\n",
        "            velocities[i] = self.beta2 * velocities[i] + (1 - self.beta2) * (gradients[i] ** 2)\n",
        "\n",
        "        # Bias corrections for moments and velocities\n",
        "        m_hats = [m / (1 - self.beta1) for m in moments]\n",
        "        v_hats = [v / (1 - self.beta2) for v in velocities]\n",
        "\n",
        "        # Update parameters using corrected estimates\n",
        "        for i in range(len(parameters)):\n",
        "            parameters[i] -= self.learning_rate * m_hats[i] / (np.sqrt(v_hats[i]) + self.epsilon)\n",
        "\n",
        "        # Update class attributes to maintain state across iterations\n",
        "        self.m_W1, self.m_b1, self.m_W2, self.m_b2 = moments\n",
        "        self.v_W1, self.v_b1, self.v_W2, self.v_b2 = velocities\n",
        "        self.W1, self.b1, self.W2, self.b2 = parameters\n",
        "\n",
        "    # Gradient descent function\n",
        "    def train(self, X_train, Y_train, X_test, Y_test, epochs=10, batch_size=64):\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "        # Shuffle training data for better generalization\n",
        "          num_batches = X_train.shape[1] // batch_size\n",
        "\n",
        "          for batch in range(num_batches):\n",
        "              # Get mini-batch of training data\n",
        "              start = batch * batch_size\n",
        "              end = (batch + 1) * batch_size\n",
        "              X_batch = X_train[:, start:end]\n",
        "              Y_batch = Y_train[:, start:end]\n",
        "\n",
        "              # Perform forward propagation\n",
        "              Y_hat = self.forward_propagation(X_batch)\n",
        "\n",
        "              # Calculate loss with mini-batch\n",
        "              loss = self.categorical_cross_entropy(Y_batch, Y_hat)\n",
        "\n",
        "              # Backward propagation to get gradients\n",
        "              dW1, db1, dW2, db2 = self.backward_propagation(X_batch, Y_batch, Y_hat)\n",
        "\n",
        "              # Update parameters with Adam optimizer\n",
        "              self.adam_optimizer(dW1, db1, dW2, db2)\n",
        "\n",
        "          # Calculate accuracy for current epoch\n",
        "          Y_pred = self.predict(X_test)\n",
        "          Y_actual = np.argmax(Y_test, axis=0)\n",
        "          accuracy = np.mean(Y_pred == Y_actual) * 100\n",
        "\n",
        "          # Print the loss and accuracy at the end of each epoch\n",
        "          print(f\"Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "    # Function that gets the predictions for the model on inputted data\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward_propagation(X), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s48C_1kovgpb"
      },
      "source": [
        "To verify that my model was working correctly, I assigned the input parameters to variables, created a NeuralNetwork object, and trained it on the training set. I determined the hidden layer size and learning rate through trial and error during debugging, which resulted in good performance and kept the training time reasonable. I ran the model for 15 epochs to allow sufficient time for convergence and printed the loss and accuracy at each epoch to monitor its progress in real time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swiHVafIeSIJ",
        "outputId": "6c843e45-990a-40cf-ab27-28e65b542bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3547616778608269, Accuracy: 94.46%\n",
            "Epoch 1, Loss: 0.32725486594891884, Accuracy: 95.56%\n",
            "Epoch 2, Loss: 0.31266779973693426, Accuracy: 96.16%\n",
            "Epoch 3, Loss: 0.3001264506697317, Accuracy: 96.52%\n",
            "Epoch 4, Loss: 0.28867739800733405, Accuracy: 96.86%\n",
            "Epoch 5, Loss: 0.2792358683392269, Accuracy: 97.02%\n",
            "Epoch 6, Loss: 0.2709056031416237, Accuracy: 97.13%\n",
            "Epoch 7, Loss: 0.2633955994250666, Accuracy: 97.19%\n",
            "Epoch 8, Loss: 0.2533194470871217, Accuracy: 97.29%\n",
            "Epoch 9, Loss: 0.24417237831647562, Accuracy: 97.35%\n",
            "Epoch 10, Loss: 0.2343513580678401, Accuracy: 97.45%\n",
            "Epoch 11, Loss: 0.22455291097411886, Accuracy: 97.52%\n",
            "Epoch 12, Loss: 0.215267781916624, Accuracy: 97.59%\n",
            "Epoch 13, Loss: 0.2053334018259039, Accuracy: 97.63%\n",
            "Epoch 14, Loss: 0.196132603738197, Accuracy: 97.62%\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "input_size = 784       # For MNIST (28x28 images)\n",
        "hidden_size = 128      # First hidden layer neurons\n",
        "output_size = 10       # Number of output classes (digits 0-9)\n",
        "learning_rate = 0.001  # Learning rate for Adam optimizer\n",
        "\n",
        "# Initialize the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 15            # More epochs for better convergence\n",
        "batch_size = 64        # Mini-batch size\n",
        "\n",
        "# Train the model\n",
        "nn.train(train_X, train_y, test_X, test_y, epochs=epochs, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKV6ujr6zW1w"
      },
      "source": [
        "In the following code, I demonstrate that my trained model achieves a high accuracy score on the test data using my predict() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuyiw__qzkkN",
        "outputId": "3530c343-5601-4e45-90c6-6de2427b6927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [7 2 1 ... 4 5 6]\n",
            "Actual: [7 2 1 ... 4 5 6]\n",
            "Accuracy: 97.61999999999999%\n"
          ]
        }
      ],
      "source": [
        "# Generating predictions for the test set\n",
        "predictions = nn.predict(test_X)\n",
        "actual = np.argmax(test_y, axis=0)\n",
        "accuracy = np.mean(predictions == actual) * 100\n",
        "\n",
        "# Printing the predictions side by side and the accuracy\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual:\", actual)\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNr54ZanAmIc"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN_DP5rk0ffm"
      },
      "source": [
        "## Task 1: Materials Used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9JN7iWt0rKJ"
      },
      "source": [
        "I chose to use TensorFlow as my deep learning library to implement my two-hidden-layer neural network because it is a widely recognized framework with easy-to-use functions for creating, training, and testing models.\n",
        "\n",
        "The first resource I used was the official documentation on \"The Sequential Model\" from TensorFlow. This mini-tutorial explains how to create a neural network using the Sequential model, which is \"appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\" This approach perfectly fits the needs of my project. Additionally, it introduced me to Keras, TensorFlow's high-level API that dramatically simplifies the syntax and makes building neural networks approachable. Essentially, you create a model object, define the neuron layers (specifying the number of layers and their activation functions), compile the model by choosing an optimizer and a loss function, and then train and optimize the model using straightforward function calls.\n",
        "\n",
        "Source: https://www.tensorflow.org/guide/keras/sequential_model\n",
        "\n",
        "The next resource I used was also from the official TensorFlow documentation. It provides in-depth examples on how to train and evaluate a neural network using functions like .fit() and .evaluate(). It also describes .predict(), which performs a forward propagation step to generate predictions without updating any model weights.\n",
        "\n",
        "Source: https://www.tensorflow.org/guide/keras/training_with_built_in_methods\n",
        "\n",
        "Another valuable resource was the official TensorFlow documentation on optimizers. I referred to this to understand how to implement and compare the performance of different optimizers—Momentum, RMSProp, and Adam—in my experiments.\n",
        "\n",
        "Source: https://keras.io/api/optimizers/\n",
        "\n",
        "The final resource I used extensively was the sklearn documentation. Although TensorFlow's .evaluate() function provides basic performance measures, I wanted more detailed metrics. Sklearn's classification report computes and presents accuracy, precision, recall, and F1-score in an easy-to-digest table, which was extremely useful for displaying the results of my hyperparameter experiments.\n",
        "\n",
        "Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6zij3OS1124"
      },
      "source": [
        "## Task 2: Implementing 2 Hidden-Layer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsa8SaNQ13oC"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyjaVa7a3Meq"
      },
      "source": [
        "As done earlier, we will be importing the MNIST database as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qPB3hlWK3axv"
      },
      "outputs": [],
      "source": [
        "# Loading MNIST dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VCyaHCm3bOM"
      },
      "source": [
        "To get a better feel for the data, we can print out the shapes of the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QspUTIJP3ejI",
        "outputId": "b5a35898-133d-45e4-a238-fb74b405c668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Test data shape: (10000, 28, 28)\n",
            "Training labels shape: (60000,)\n",
            "Test labels shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# Printing data shapes\n",
        "print(f\"Training data shape: {train_X.shape}\")\n",
        "print(f\"Test data shape: {test_X.shape}\")\n",
        "print(f\"Training labels shape: {train_y.shape}\")\n",
        "print(f\"Test labels shape: {test_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq-OQI1V3sbK"
      },
      "source": [
        "Conveniently, MNIST does not contain any missing values, making preprocessing very simple. We can confirm that there are no missing values as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG0wsPBH3zZ7",
        "outputId": "505a644b-035f-462c-e5dd-eb9fa0e05fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any missing values? 0\n"
          ]
        }
      ],
      "source": [
        "# Checking if there are any missing values\n",
        "print(f\"Any missing values? {np.isnan(train_X).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymaxkY9L4OCF"
      },
      "source": [
        "To better understand what the model will be training on, we should print some of the test images to see what they look like. According to the data shape, they should be 28x28 pixels. We also know that they should be grayscale images. We can confirm this using matplotlib as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Nnc4PiVl4i8l",
        "outputId": "4a792efe-219d-4fa6-f7c4-019cca59e97b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting a few test images using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(train_X[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f\"Label: {train_y[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkUVmzqF4vN_"
      },
      "source": [
        "We can also check the distribution of the labels. This is important as it can reveal if there is any bias in the data with certain labels being overrepresented. We can do this using a histogram as performed below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "B9_RWFre5Enh",
        "outputId": "a42ad4b9-a1a9-4eb3-e3eb-2c4e130ccf67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(train_y, bins=np.arange(11)-0.5, edgecolor='black', rwidth=0.8)\n",
        "plt.xlabel('Digit')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Digit Labels')\n",
        "plt.xticks(range(10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the distribution of digit labels is relatively balanced. As a result, I do not think that it is necessary to add/remove examples to/from the dataset."
      ],
      "metadata": {
        "id": "GZSkFUJV5xMe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si03bBJG5hg7"
      },
      "source": [
        "Next, we can analyze the pixel intensity values of the image. This can be done using a histogram to visualize the distribution of the intensity values across the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "RfpEw40H59xr",
        "outputId": "f67b1adc-fa9b-437f-df38-83a4b360dfad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel value range: 0 to 255\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print the min and max pixel intensity values\n",
        "print(f\"Pixel value range: {train_X.min()} to {train_X.max()}\")\n",
        "\n",
        "# Flatten the test images to get array of intensity values\n",
        "intensity_values = train_X.flatten()\n",
        "\n",
        "# Create a histogram showing the distribution of pixel intensities\n",
        "plt.hist(intensity_values, bins=256, range=(0, 255), fc='black', ec='black')\n",
        "plt.title('Pixel Intensity Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Remove the 0 values from the array then print the histogram again\n",
        "intensity_values = intensity_values[intensity_values != 0]\n",
        "plt.hist(intensity_values, bins=256, range=(0, 255), fc='black', ec='black')\n",
        "plt.title('Pixel Intensity Distribution (Excluding Intensity 0)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMFHQSz18isV"
      },
      "source": [
        "The first histogram clearly shows that most pixel intensities in the dataset are 0, which obscures the distribution of the remaining values. To better visualize the non-zero data, I created a second histogram that excludes the 0 values. This revised plot reveals that most of the positive intensity values are clustered near the maximum value of 255, indicating that the pixel intensities are almost binary—predominantly at the lower bound (0) or near the upper bound (255)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO91slwr6Gq4"
      },
      "source": [
        "Lastly, we can check if there are any duplicate images in the training set. We can do this using a DataFrame from the pandas library. It can be seen that there are no duplicates, again making preprocessing very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdIm3Zm6Vxt",
        "outputId": "7ba9d58c-6d5b-4e90-a54d-ffeffe5a8050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate images: 0\n"
          ]
        }
      ],
      "source": [
        "duplicates = pd.DataFrame(train_X.reshape(train_X.shape[0], -1)).duplicated().sum()\n",
        "print(f\"Number of duplicate images: {duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYP-deL0182M"
      },
      "source": [
        "### Performing a Train-Dev-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5xjY0du_NJU"
      },
      "source": [
        "Since the MNIST dataset already comes with predefined training and test sets, the remaining step is to carve out a portion of the training data to serve as a development (dev) set. I allocated 15% of the training set for this purpose, resulting in a dev set of 9,000 examples, which is comparable in size to the test set. This dev set will be used to fine-tune hyperparameters and evaluate model performance before final testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4s2Oq-S92W4",
        "outputId": "517580f7-3f2a-416b-89ec-958bfabf164d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shapes:\n",
            "Training SetX: (51000, 28, 28)\n",
            "Training SetY: (51000,)\n",
            "Dev SetX: (9000, 28, 28)\n",
            "Dev SetY: (9000,)\n",
            "Test SetX: (10000, 28, 28)\n",
            "Test SetY: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# Splitting the training data into training and dev\n",
        "train_X_split, dev_X, train_y_split, dev_y = train_test_split(train_X, train_y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Print the shapes of all the datasets\n",
        "print(\"Dataset Shapes:\")\n",
        "print(f\"Training SetX: {train_X_split.shape}\")\n",
        "print(f\"Training SetY: {train_y_split.shape}\")\n",
        "print(f\"Dev SetX: {dev_X.shape}\")\n",
        "print(f\"Dev SetY: {dev_y.shape}\")\n",
        "print(f\"Test SetX: {test_X.shape}\")\n",
        "print(f\"Test SetY: {test_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBILXqPOC6PU"
      },
      "source": [
        "Like in Part 1, I also want collapse the 28x28 test images into 784 vectors to be used in the neural network. This is done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb0FSCCsDGEX",
        "outputId": "da896274-6ef3-4d55-f313-95813dcf2fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (51000, 784)\n",
            "X_test:  (10000, 784)\n",
            "X_dev:  (9000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Ensure that the training data is in the shape (features, examples)\n",
        "train_X_split = train_X_split.reshape(train_X_split.shape[0], -1)\n",
        "test_X = test_X.reshape(test_X.shape[0], -1)\n",
        "dev_X = dev_X.reshape(dev_X.shape[0], -1)\n",
        "\n",
        "# Print the shapes to confirm\n",
        "print('X_train: ' + str(train_X_split.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('X_dev:  '  + str(dev_X.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcLNz5Zd2ARt"
      },
      "source": [
        "### Implementing Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb2UCkRFBG7X"
      },
      "source": [
        "The next step is to implement forward propagation. Rather than building the neural network from scratch, I will use TensorFlow, a widely recognized deep learning framework. Additionally, I will utilize Keras, TensorFlow's high-level API, which simplifies syntax and enhances readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PkF7PlzxAipw"
      },
      "outputs": [],
      "source": [
        "# Define the shape of the model\n",
        "input_shape = (train_X_split.shape[1],)\n",
        "\n",
        "# Creating the neural network model with TensorFlow\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the Adam optimizer into the network\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6HOZMerB9D-"
      },
      "source": [
        "The above code defines and compiles a neural network model. The model is sequential, consisting of two hidden layers with 64 neurons each, using the ReLU activation function, followed by an output layer with 10 neurons and a softmax activation to classify the MNIST digits. The input shape is based on the dimensions of the training data. The model is compiled with the Adam optimizer, using sparse categorical crossentropy as the loss function, and accuracy as the evaluation metric.\n",
        "\n",
        "I chose to use ReLU, softmax, and adam for the same reasons that I used them for Part 1, as summarized below:\n",
        "* ReLU is used because it introduces non-linearity, enabling the model to learn complex patterns and mitigating the vanishing gradient problem for faster training.\n",
        "* Softmax is ideal for multi-class classification as it converts output scores into probabilities, simplifying interpretation of predictions.\n",
        "* The Adam optimizer was chosen for its adaptive learning rate and efficient performance, leading to faster convergence.\n",
        "\n",
        "I switched to the sparse categorical cross-entropy loss function for my MNIST classification neural network because it is ideal for multi-class classification tasks where labels are integers instead of one-hot encoded vectors. In the MNIST dataset, each image is labeled with an integer from 0 to 9. This loss function directly compares the model's predicted probabilities with these integer labels, simplifying the implementation by removing the need to convert labels into one-hot encoded format.\n",
        "\n",
        "Source:\n",
        "* https://medium.com/@shireenchand/choosing-between-cross-entropy-and-sparse-cross-entropy-the-only-guide-you-need-abea92c84662\n",
        "\n",
        "With the model created, I can now perform forward propagation using the following code. We can do some investigation by printing out performance metrics for the untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXf3s5bbFK6K",
        "outputId": "b0dc7f1f-18af-4bca-8a23-5b8710942c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.03      0.05       891\n",
            "           1       0.12      0.98      0.21       991\n",
            "           2       0.00      0.00      0.00       896\n",
            "           3       0.05      0.01      0.02       904\n",
            "           4       0.06      0.00      0.01       872\n",
            "           5       0.03      0.01      0.01       827\n",
            "           6       0.00      0.00      0.00       878\n",
            "           7       0.00      0.00      0.00       963\n",
            "           8       0.12      0.01      0.02       865\n",
            "           9       0.12      0.00      0.00       913\n",
            "\n",
            "    accuracy                           0.11      9000\n",
            "   macro avg       0.07      0.10      0.03      9000\n",
            "weighted avg       0.07      0.11      0.03      9000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Get predictions\n",
        "predictions = model.predict(dev_X)\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(dev_y, predicted_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPkzGsRIdn2"
      },
      "source": [
        "We can see that the scores for precision, recall, f1-score and accuracy are not good whatsoever. This is to be expected because we are making predictions without doing any training yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQtGZ2Z-2uEi"
      },
      "source": [
        "### Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAHeu3Y0JPFI"
      },
      "source": [
        "Now its time to implement gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLJ-A6FtMldd",
        "outputId": "93ad341d-f5fc-408e-acf4-4b5aa96f78f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6119 - loss: 17.7836 - val_accuracy: 0.8451 - val_loss: 1.4409\n",
            "Epoch 2/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8698 - loss: 1.0894 - val_accuracy: 0.8702 - val_loss: 0.8631\n",
            "Epoch 3/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8912 - loss: 0.6298 - val_accuracy: 0.8959 - val_loss: 0.5842\n",
            "Epoch 4/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9143 - loss: 0.4180 - val_accuracy: 0.9078 - val_loss: 0.4847\n",
            "Epoch 5/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9275 - loss: 0.3095 - val_accuracy: 0.9148 - val_loss: 0.4225\n",
            "Epoch 6/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9373 - loss: 0.2494 - val_accuracy: 0.9128 - val_loss: 0.4304\n",
            "Epoch 7/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9428 - loss: 0.2205 - val_accuracy: 0.9207 - val_loss: 0.3954\n",
            "Epoch 8/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9477 - loss: 0.1924 - val_accuracy: 0.9303 - val_loss: 0.3396\n",
            "Epoch 9/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9528 - loss: 0.1723 - val_accuracy: 0.9290 - val_loss: 0.3310\n",
            "Epoch 10/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9593 - loss: 0.1420 - val_accuracy: 0.9270 - val_loss: 0.3493\n",
            "Epoch 11/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9583 - loss: 0.1475 - val_accuracy: 0.9298 - val_loss: 0.3281\n",
            "Epoch 12/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9607 - loss: 0.1277 - val_accuracy: 0.9372 - val_loss: 0.3116\n",
            "Epoch 13/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9636 - loss: 0.1260 - val_accuracy: 0.9399 - val_loss: 0.3199\n",
            "Epoch 14/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9661 - loss: 0.1172 - val_accuracy: 0.9401 - val_loss: 0.3004\n",
            "Epoch 15/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.1083 - val_accuracy: 0.9371 - val_loss: 0.3251\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "losses = model.fit(train_X_split, train_y_split,\n",
        "                   validation_data=(dev_X, dev_y),\n",
        "                   batch_size=256,\n",
        "                   epochs=15,\n",
        "                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAR-7u7WNZZ8"
      },
      "source": [
        "We can print the loss calculations as the decrease throughout the epochs, resulting in the final computation of the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N4I8-UlNgOh",
        "outputId": "ae8746d7-dd19-4206-a6d5-dfab5cd41e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array of losses [6.14089298248291, 0.9773024916648865, 0.5853509306907654, 0.4135166108608246, 0.3173213601112366, 0.2600937485694885, 0.23022332787513733, 0.19346584379673004, 0.1694423258304596, 0.1544092446565628, 0.14570283889770508, 0.13265559077262878, 0.12797583639621735, 0.12361510097980499, 0.11371169239282608]\n",
            "Final loss value: 0.11371169239282608\n"
          ]
        }
      ],
      "source": [
        "# Printing the losses calculated throughout training\n",
        "print(f\"Array of losses {losses.history['loss']}\")\n",
        "print(f\"Final loss value: {losses.history['loss'][-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnxSxnxNOuhW"
      },
      "source": [
        "### Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYleOQV8PE4T"
      },
      "source": [
        "#### Different optimization algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oddUbyQPfqG"
      },
      "source": [
        "Although I used the adam optimizer in my original implementation, I simply assumed it would be the best performing since its includes the characteristics of both Momentum and RMSProp. However, it is worth testing the network with the other two optimizers to see if they produce better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w5V6JwNQYAC",
        "outputId": "884885ca-61bb-4665-bc56-823a28513b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: Adam\n",
            "Test Accuracy: 0.9608\n",
            "Test Loss: 0.1638\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Classification Report for Adam:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       891\n",
            "           1       0.98      0.99      0.98       991\n",
            "           2       0.94      0.95      0.95       896\n",
            "           3       0.94      0.96      0.95       904\n",
            "           4       0.94      0.98      0.96       872\n",
            "           5       0.97      0.95      0.96       827\n",
            "           6       0.99      0.95      0.97       878\n",
            "           7       0.95      0.98      0.96       963\n",
            "           8       0.97      0.94      0.95       865\n",
            "           9       0.96      0.92      0.94       913\n",
            "\n",
            "    accuracy                           0.96      9000\n",
            "   macro avg       0.96      0.96      0.96      9000\n",
            "weighted avg       0.96      0.96      0.96      9000\n",
            "\n",
            "Optimizer: Momentum\n",
            "Test Accuracy: 0.1101\n",
            "Test Loss: 2.3017\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Classification Report for Momentum:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       891\n",
            "           1       0.11      1.00      0.20       991\n",
            "           2       0.00      0.00      0.00       896\n",
            "           3       0.00      0.00      0.00       904\n",
            "           4       0.00      0.00      0.00       872\n",
            "           5       0.00      0.00      0.00       827\n",
            "           6       0.00      0.00      0.00       878\n",
            "           7       0.00      0.00      0.00       963\n",
            "           8       0.00      0.00      0.00       865\n",
            "           9       0.00      0.00      0.00       913\n",
            "\n",
            "    accuracy                           0.11      9000\n",
            "   macro avg       0.01      0.10      0.02      9000\n",
            "weighted avg       0.01      0.11      0.02      9000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: RMSProp\n",
            "Test Accuracy: 0.9471\n",
            "Test Loss: 0.3387\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Classification Report for RMSProp:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96       891\n",
            "           1       0.98      0.96      0.97       991\n",
            "           2       0.93      0.96      0.95       896\n",
            "           3       0.94      0.94      0.94       904\n",
            "           4       0.99      0.92      0.95       872\n",
            "           5       0.96      0.91      0.93       827\n",
            "           6       0.95      0.97      0.96       878\n",
            "           7       0.96      0.96      0.96       963\n",
            "           8       0.84      0.96      0.90       865\n",
            "           9       0.96      0.92      0.94       913\n",
            "\n",
            "    accuracy                           0.95      9000\n",
            "   macro avg       0.95      0.95      0.95      9000\n",
            "weighted avg       0.95      0.95      0.95      9000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function used to test Momentum, RMSProp and Adam optimizers\n",
        "def test_optimizers(train_X, train_y, test_X, test_y):\n",
        "    # Create optimizer objects and store their names\n",
        "    optimizers = [tf.keras.optimizers.Adam(),\n",
        "                  tf.keras.optimizers.SGD(momentum=0.9),\n",
        "                  tf.keras.optimizers.RMSprop()]\n",
        "    optimizer_names = ['Adam', 'Momentum', 'RMSProp']\n",
        "\n",
        "    # Define the shape of the input data\n",
        "    input_shape = (train_X.shape[1],)\n",
        "\n",
        "    # Train and evaluate each optimizer\n",
        "    for optimizer, name in zip(optimizers, optimizer_names):\n",
        "        # Initialize a new model instance for each optimizer\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=input_shape),\n",
        "            tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "            tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compile the model with the current optimizer\n",
        "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(train_X, train_y, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
        "        print(f\"Optimizer: {name}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = model.predict(test_X)\n",
        "        predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "        # Classification report\n",
        "        print(f\"Classification Report for {name}:\")\n",
        "        print(classification_report(test_y, predicted_classes))\n",
        "\n",
        "# Run the function\n",
        "test_optimizers(train_X_split, train_y_split, dev_X, dev_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQFa2qrKVT1h"
      },
      "source": [
        "The results indicate that the model performed best with Adam and RMSProp optimizers, both achieving high test accuracy and low test loss with consistently strong classification metrics across all classes. In contrast, using the Momentum optimizer resulted in extremely poor performance, with very low accuracy and high loss, suggesting that it failed to learn effectively—likely due to suboptimal hyperparameters. Overall, these findings suggest that for this MNIST classification task, Adam and RMSProp are much more effective than Momentum.\n",
        "\n",
        "For my final implementation, I will continue to use Adam as it has all the functionality of RMSProp with additional functionality. Additionally, Adam's added complexity does not seem to be negatively impacting the training time of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGEzS6ktPLV1"
      },
      "source": [
        "#### Different numbers of hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also wanted to test different numbers of hidden layers to see if performance could be improved. With more hidden layers, more complex relationships can be captured by the model, however, the risk of overfitting becomes greater. The following is the code I used to test different hidden layer structures."
      ],
      "metadata": {
        "id": "UW4KBWNe4x3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p58QDD7HWGJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977311c7-2cb1-4649-b6f8-95fc58bb19fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.2688\n",
            "Number of Hidden Layers: 1\n",
            "Test Accuracy: 0.9327\n",
            "Test Loss: 0.2764\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report for 1 Hidden Layers:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96       891\n",
            "           1       0.98      0.97      0.97       991\n",
            "           2       0.92      0.92      0.92       896\n",
            "           3       0.93      0.89      0.91       904\n",
            "           4       0.97      0.92      0.94       872\n",
            "           5       0.97      0.87      0.92       827\n",
            "           6       0.95      0.96      0.96       878\n",
            "           7       0.97      0.93      0.95       963\n",
            "           8       0.80      0.94      0.87       865\n",
            "           9       0.88      0.96      0.92       913\n",
            "\n",
            "    accuracy                           0.93      9000\n",
            "   macro avg       0.94      0.93      0.93      9000\n",
            "weighted avg       0.94      0.93      0.93      9000\n",
            "\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1790\n",
            "Number of Hidden Layers: 2\n",
            "Test Accuracy: 0.9580\n",
            "Test Loss: 0.1688\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Classification Report for 2 Hidden Layers:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98       891\n",
            "           1       0.99      0.99      0.99       991\n",
            "           2       0.98      0.94      0.96       896\n",
            "           3       0.93      0.96      0.95       904\n",
            "           4       0.93      0.98      0.95       872\n",
            "           5       0.98      0.92      0.95       827\n",
            "           6       0.99      0.96      0.98       878\n",
            "           7       0.97      0.96      0.97       963\n",
            "           8       0.90      0.96      0.93       865\n",
            "           9       0.94      0.94      0.94       913\n",
            "\n",
            "    accuracy                           0.96      9000\n",
            "   macro avg       0.96      0.96      0.96      9000\n",
            "weighted avg       0.96      0.96      0.96      9000\n",
            "\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1514\n",
            "Number of Hidden Layers: 3\n",
            "Test Accuracy: 0.9616\n",
            "Test Loss: 0.1543\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report for 3 Hidden Layers:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97       891\n",
            "           1       0.99      0.99      0.99       991\n",
            "           2       0.95      0.95      0.95       896\n",
            "           3       0.96      0.94      0.95       904\n",
            "           4       0.95      0.97      0.96       872\n",
            "           5       0.95      0.97      0.96       827\n",
            "           6       0.98      0.97      0.98       878\n",
            "           7       0.95      0.98      0.97       963\n",
            "           8       0.96      0.92      0.94       865\n",
            "           9       0.93      0.96      0.94       913\n",
            "\n",
            "    accuracy                           0.96      9000\n",
            "   macro avg       0.96      0.96      0.96      9000\n",
            "weighted avg       0.96      0.96      0.96      9000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function used to test different numbers of hidden layers\n",
        "def test_layers(train_X, train_y, test_X, test_y):\n",
        "    input_shape = (train_X.shape[1],)\n",
        "    layers = [1, 2, 3]\n",
        "\n",
        "    for layer_count in layers:\n",
        "        # Initialize a new model\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.Input(shape=input_shape))\n",
        "\n",
        "        # Add the specified number of hidden layers\n",
        "        for _ in range(layer_count):\n",
        "            model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(train_X, train_y, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=1)\n",
        "        print(f\"Number of Hidden Layers: {layer_count}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = model.predict(test_X)\n",
        "        predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "        # Classification report\n",
        "        print(f\"Classification Report for {layer_count} Hidden Layers:\")\n",
        "        print(classification_report(test_y, predicted_classes))\n",
        "\n",
        "# Run the function\n",
        "test_layers(train_X_split, train_y_split, dev_X, dev_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our tests, the model with three hidden layers achieved the best overall performance, yielding higher test accuracy and lower test loss than the one- and two-hidden-layer configurations. The classification report for the three-hidden-layer model also showed consistently strong performance across all digit classes, suggesting that the extra depth helped capture more complex patterns and improved generalization."
      ],
      "metadata": {
        "id": "RKaDIOXvcaHs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwBp_mgCPPBJ"
      },
      "source": [
        "#### Normalizing pixel intensities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I did in Part 1, I normalized the pixel intensity values to a range between 0 and 1. In theory, this normalization should speed up the training process and yield better results by ensuring that the input features are on a similar scale. However, for training on MNIST, we can perform an experiment to determine whether normalization actually makes a significant impact on performance."
      ],
      "metadata": {
        "id": "iZkZC739cq9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to build and compile the model\n",
        "def create_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Defining the input shape\n",
        "input_shape = (train_X_split.shape[1],)\n",
        "\n",
        "# Train on unnormalized data\n",
        "print(\"Training on unnormalized data:\")\n",
        "model1 = create_model(input_shape)\n",
        "model1.fit(train_X_split, train_y_split, epochs=10, batch_size=32, verbose=0)\n",
        "test_loss, test_accuracy = model1.evaluate(dev_X, dev_y, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "predictions = model1.predict(dev_X)\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "print(\"Classification Report for unnormalized data:\")\n",
        "print(classification_report(dev_y, predicted_classes))\n",
        "\n",
        "# Train on normalized data (pixel values scaled to [0,1])\n",
        "print(\"\\nTraining on normalized data:\")\n",
        "X_train_norm = train_X_split / 255.0\n",
        "X_dev_norm = dev_X / 255.0\n",
        "model2 = create_model(input_shape)\n",
        "model2.fit(X_train_norm, train_y_split, epochs=10, batch_size=32, verbose=0)\n",
        "test_loss, test_accuracy = model2.evaluate(X_dev_norm, dev_y, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "predictions = model2.predict(X_dev_norm)\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "print(\"Classification Report for normalized data:\")\n",
        "print(classification_report(dev_y, predicted_classes))\n",
        "\n"
      ],
      "metadata": {
        "id": "ciMSeUI3c5wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8f5b98-7da8-4097-f478-10a7cc1d291b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on unnormalized data:\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.1670\n",
            "Test Accuracy: 0.9650\n",
            "Test Loss: 0.1486\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Classification Report for unnormalized data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       891\n",
            "           1       0.99      0.98      0.99       991\n",
            "           2       0.96      0.97      0.96       896\n",
            "           3       0.96      0.96      0.96       904\n",
            "           4       0.95      0.98      0.96       872\n",
            "           5       0.97      0.95      0.96       827\n",
            "           6       0.97      0.97      0.97       878\n",
            "           7       0.98      0.96      0.97       963\n",
            "           8       0.93      0.95      0.94       865\n",
            "           9       0.95      0.95      0.95       913\n",
            "\n",
            "    accuracy                           0.96      9000\n",
            "   macro avg       0.96      0.96      0.96      9000\n",
            "weighted avg       0.97      0.96      0.97      9000\n",
            "\n",
            "\n",
            "Training on normalized data:\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9729 - loss: 0.0997\n",
            "Test Accuracy: 0.9744\n",
            "Test Loss: 0.0972\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report for normalized data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       891\n",
            "           1       0.99      0.99      0.99       991\n",
            "           2       0.98      0.97      0.97       896\n",
            "           3       0.98      0.95      0.96       904\n",
            "           4       0.98      0.98      0.98       872\n",
            "           5       0.94      0.98      0.96       827\n",
            "           6       0.98      0.99      0.98       878\n",
            "           7       0.98      0.98      0.98       963\n",
            "           8       0.97      0.96      0.97       865\n",
            "           9       0.96      0.97      0.96       913\n",
            "\n",
            "    accuracy                           0.97      9000\n",
            "   macro avg       0.97      0.97      0.97      9000\n",
            "weighted avg       0.97      0.97      0.97      9000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the experiment we can conclude that the model trained on the normalized data performed better. Normalizing the pixel values led to higher test accuracy, lower test loss, and improved classification metrics (precision, recall, and F1-scores) across all classes compared to training on unnormalized data."
      ],
      "metadata": {
        "id": "vm2l8P9Ef0Ky"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBWQ5JIY22j2"
      },
      "source": [
        "### Presenting the Final Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After all the experimentation, we can combine the best performing characteristics and test the network on the test dataset. The following neural network will have:\n",
        "\n",
        "* 3 hidden layers\n",
        "* Adam optimization algorithm\n",
        "* Normalizing the data\n",
        "\n",
        "Finally we can move on from the dev dataset and see how the model performs on new data!"
      ],
      "metadata": {
        "id": "u1ISTWFmlQV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "X_tr = train_X_split / 255.0\n",
        "X_ts = test_X / 255.0\n",
        "\n",
        "# Initialize a new model instance for each optimizer\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with the current optimizer\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_tr, train_y_split, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_ts, test_y, verbose=1)\n",
        "predictions = model.predict(X_ts)\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_y, predicted_classes))"
      ],
      "metadata": {
        "id": "GYpOVA8VlrfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e954184-e559-4680-a80d-814ea4d801f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.5152\n",
            "Epoch 2/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1362\n",
            "Epoch 3/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9697 - loss: 0.0952\n",
            "Epoch 4/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0733\n",
            "Epoch 5/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9826 - loss: 0.0557\n",
            "Epoch 6/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.0496\n",
            "Epoch 7/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9856 - loss: 0.0425\n",
            "Epoch 8/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.0349\n",
            "Epoch 9/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0321\n",
            "Epoch 10/10\n",
            "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9915 - loss: 0.0259\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1124\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.98      0.97      0.97      1032\n",
            "           3       0.98      0.97      0.97      1010\n",
            "           4       0.97      0.97      0.97       982\n",
            "           5       0.98      0.97      0.97       892\n",
            "           6       0.97      0.99      0.98       958\n",
            "           7       0.96      0.99      0.97      1028\n",
            "           8       0.97      0.96      0.96       974\n",
            "           9       0.97      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.98      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the best hyperparameters, the resulting model achieved strong performance on the MNIST dataset, with approximately 97% accuracy and excellent precision, recall, and F1-scores across all classes. While the basic neural network proved to be an effective choice for this task, certain simplifications—such as flattening the 28x28 images—limit its ability to capture the spatial relationships between pixels. Advanced techniques, like convolutional neural networks, preserve the original image structure and can better exploit these spatial relationships, potentially leading to even greater performance improvements."
      ],
      "metadata": {
        "id": "cxd7pGKM6aHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statement About Use of Generative AI"
      ],
      "metadata": {
        "id": "aOj3uOnuB4zJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout this project, I leveraged generative AI models to help refine the structure and grammar of my writing. I also used them to format some of my code for better readability and consulted them for guidance on which TensorFlow functions, and other functions to use before researching and implementing them myself. All content presented is my own, and any external sources referenced for this assignment are properly cited or linked where appropriate. I wasn't sure of the exact citation format for AI usage, so I felt this paragraph would suffice.\n",
        "\n",
        "I thoroughly enjoyed completing this assignment and deepening my understanding of so many new concepts!"
      ],
      "metadata": {
        "id": "3-D7nRHyCMfj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnRRpck6dO0oOQOUVI5bEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}